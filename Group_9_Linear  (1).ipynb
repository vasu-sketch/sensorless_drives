{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e220b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cc404f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current_1</th>\n",
       "      <th>current_2</th>\n",
       "      <th>current_3</th>\n",
       "      <th>current_4</th>\n",
       "      <th>current_5</th>\n",
       "      <th>current_6</th>\n",
       "      <th>current_7</th>\n",
       "      <th>current_8</th>\n",
       "      <th>current_9</th>\n",
       "      <th>current_10</th>\n",
       "      <th>...</th>\n",
       "      <th>current_40</th>\n",
       "      <th>current_41</th>\n",
       "      <th>current_42</th>\n",
       "      <th>current_43</th>\n",
       "      <th>current_44</th>\n",
       "      <th>current_45</th>\n",
       "      <th>current_46</th>\n",
       "      <th>current_47</th>\n",
       "      <th>current_48</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.014600e-07</td>\n",
       "      <td>8.260300e-06</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-1.438600e-06</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.031718</td>\n",
       "      <td>0.031710</td>\n",
       "      <td>0.031721</td>\n",
       "      <td>-0.032963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.63308</td>\n",
       "      <td>2.9646</td>\n",
       "      <td>8.1198</td>\n",
       "      <td>-1.4961</td>\n",
       "      <td>-1.4961</td>\n",
       "      <td>-1.4961</td>\n",
       "      <td>-1.4996</td>\n",
       "      <td>-1.4996</td>\n",
       "      <td>-1.4996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.913200e-06</td>\n",
       "      <td>-5.247700e-06</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>2.778900e-06</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.030804</td>\n",
       "      <td>0.030810</td>\n",
       "      <td>0.030806</td>\n",
       "      <td>-0.033520</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.59314</td>\n",
       "      <td>7.6252</td>\n",
       "      <td>6.1690</td>\n",
       "      <td>-1.4967</td>\n",
       "      <td>-1.4967</td>\n",
       "      <td>-1.4967</td>\n",
       "      <td>-1.5005</td>\n",
       "      <td>-1.5005</td>\n",
       "      <td>-1.5005</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.951700e-06</td>\n",
       "      <td>-3.184000e-06</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-1.575300e-06</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.032877</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.032896</td>\n",
       "      <td>-0.029834</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.63252</td>\n",
       "      <td>2.7784</td>\n",
       "      <td>5.3017</td>\n",
       "      <td>-1.4983</td>\n",
       "      <td>-1.4983</td>\n",
       "      <td>-1.4982</td>\n",
       "      <td>-1.4985</td>\n",
       "      <td>-1.4985</td>\n",
       "      <td>-1.4985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.322600e-06</td>\n",
       "      <td>8.820100e-06</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-7.282900e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.029410</td>\n",
       "      <td>0.029401</td>\n",
       "      <td>0.029417</td>\n",
       "      <td>-0.030156</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.62289</td>\n",
       "      <td>6.5534</td>\n",
       "      <td>6.2606</td>\n",
       "      <td>-1.4963</td>\n",
       "      <td>-1.4963</td>\n",
       "      <td>-1.4963</td>\n",
       "      <td>-1.4975</td>\n",
       "      <td>-1.4975</td>\n",
       "      <td>-1.4976</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6.836600e-08</td>\n",
       "      <td>5.666300e-07</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-7.940600e-07</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.030119</td>\n",
       "      <td>0.030119</td>\n",
       "      <td>0.030145</td>\n",
       "      <td>-0.031393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.63010</td>\n",
       "      <td>4.5155</td>\n",
       "      <td>9.5231</td>\n",
       "      <td>-1.4958</td>\n",
       "      <td>-1.4958</td>\n",
       "      <td>-1.4958</td>\n",
       "      <td>-1.4959</td>\n",
       "      <td>-1.4959</td>\n",
       "      <td>-1.4959</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      current_1     current_2  current_3  current_4     current_5  current_6  \\\n",
       "0 -3.014600e-07  8.260300e-06  -0.000012  -0.000002 -1.438600e-06  -0.000021   \n",
       "1  2.913200e-06 -5.247700e-06   0.000003  -0.000006  2.778900e-06  -0.000004   \n",
       "2 -2.951700e-06 -3.184000e-06  -0.000016  -0.000001 -1.575300e-06   0.000017   \n",
       "3 -1.322600e-06  8.820100e-06  -0.000016  -0.000005 -7.282900e-07   0.000004   \n",
       "4 -6.836600e-08  5.666300e-07  -0.000026  -0.000006 -7.940600e-07   0.000013   \n",
       "\n",
       "   current_7  current_8  current_9  current_10  ...  current_40  current_41  \\\n",
       "0   0.031718   0.031710   0.031721   -0.032963  ...    -0.63308      2.9646   \n",
       "1   0.030804   0.030810   0.030806   -0.033520  ...    -0.59314      7.6252   \n",
       "2   0.032877   0.032880   0.032896   -0.029834  ...    -0.63252      2.7784   \n",
       "3   0.029410   0.029401   0.029417   -0.030156  ...    -0.62289      6.5534   \n",
       "4   0.030119   0.030119   0.030145   -0.031393  ...    -0.63010      4.5155   \n",
       "\n",
       "   current_42  current_43  current_44  current_45  current_46  current_47  \\\n",
       "0      8.1198     -1.4961     -1.4961     -1.4961     -1.4996     -1.4996   \n",
       "1      6.1690     -1.4967     -1.4967     -1.4967     -1.5005     -1.5005   \n",
       "2      5.3017     -1.4983     -1.4983     -1.4982     -1.4985     -1.4985   \n",
       "3      6.2606     -1.4963     -1.4963     -1.4963     -1.4975     -1.4975   \n",
       "4      9.5231     -1.4958     -1.4958     -1.4958     -1.4959     -1.4959   \n",
       "\n",
       "   current_48  output  \n",
       "0     -1.4996       1  \n",
       "1     -1.5005       1  \n",
       "2     -1.4985       1  \n",
       "3     -1.4976       1  \n",
       "4     -1.4959       1  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Group_9_data_cleaned.csv')\n",
    "data.head(3)\n",
    "# removing a redundant column\n",
    "df = data.drop(['Unnamed: 0'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecbc5da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58509, 49)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of the data \n",
    "# The data has 58509 rows \n",
    "# The data has 49 features \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7014108c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current_1</th>\n",
       "      <th>current_2</th>\n",
       "      <th>current_3</th>\n",
       "      <th>current_4</th>\n",
       "      <th>current_5</th>\n",
       "      <th>current_6</th>\n",
       "      <th>current_7</th>\n",
       "      <th>current_8</th>\n",
       "      <th>current_9</th>\n",
       "      <th>current_10</th>\n",
       "      <th>...</th>\n",
       "      <th>current_40</th>\n",
       "      <th>current_41</th>\n",
       "      <th>current_42</th>\n",
       "      <th>current_43</th>\n",
       "      <th>current_44</th>\n",
       "      <th>current_45</th>\n",
       "      <th>current_46</th>\n",
       "      <th>current_47</th>\n",
       "      <th>current_48</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>58509.000000</td>\n",
       "      <td>5.850900e+04</td>\n",
       "      <td>5.850900e+04</td>\n",
       "      <td>5.850900e+04</td>\n",
       "      <td>5.850900e+04</td>\n",
       "      <td>5.850900e+04</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>1.756436e-06</td>\n",
       "      <td>1.111802e-06</td>\n",
       "      <td>-9.722586e-07</td>\n",
       "      <td>1.775487e-06</td>\n",
       "      <td>-7.457535e-07</td>\n",
       "      <td>0.002854</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>-0.006349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.650427</td>\n",
       "      <td>5.730645</td>\n",
       "      <td>7.702923</td>\n",
       "      <td>-1.500891</td>\n",
       "      <td>-1.500915</td>\n",
       "      <td>-1.500809</td>\n",
       "      <td>-1.497787</td>\n",
       "      <td>-1.497814</td>\n",
       "      <td>-1.497710</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>3.199783e-05</td>\n",
       "      <td>1.520782e-04</td>\n",
       "      <td>7.956279e-06</td>\n",
       "      <td>3.234750e-05</td>\n",
       "      <td>1.490893e-04</td>\n",
       "      <td>0.033692</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.033701</td>\n",
       "      <td>0.050735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100670</td>\n",
       "      <td>5.781169</td>\n",
       "      <td>4.347205</td>\n",
       "      <td>0.003629</td>\n",
       "      <td>0.003640</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>0.002984</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>3.162305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-6.427550e-05</td>\n",
       "      <td>-2.937900e-04</td>\n",
       "      <td>-1.887580e-05</td>\n",
       "      <td>-6.547550e-05</td>\n",
       "      <td>-2.915565e-04</td>\n",
       "      <td>-0.086972</td>\n",
       "      <td>-0.087041</td>\n",
       "      <td>-0.086978</td>\n",
       "      <td>-0.111281</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.902350</td>\n",
       "      <td>-0.596830</td>\n",
       "      <td>0.320660</td>\n",
       "      <td>-1.510950</td>\n",
       "      <td>-1.511200</td>\n",
       "      <td>-1.510700</td>\n",
       "      <td>-1.504700</td>\n",
       "      <td>-1.504550</td>\n",
       "      <td>-1.504450</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-1.444400e-05</td>\n",
       "      <td>-7.239600e-05</td>\n",
       "      <td>-5.417500e-06</td>\n",
       "      <td>-1.475300e-05</td>\n",
       "      <td>-7.379100e-05</td>\n",
       "      <td>-0.019927</td>\n",
       "      <td>-0.019951</td>\n",
       "      <td>-0.019925</td>\n",
       "      <td>-0.032144</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.715470</td>\n",
       "      <td>1.450300</td>\n",
       "      <td>4.436300</td>\n",
       "      <td>-1.503300</td>\n",
       "      <td>-1.503400</td>\n",
       "      <td>-1.503200</td>\n",
       "      <td>-1.499600</td>\n",
       "      <td>-1.499600</td>\n",
       "      <td>-1.499500</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>8.804600e-07</td>\n",
       "      <td>5.137700e-07</td>\n",
       "      <td>-1.059100e-06</td>\n",
       "      <td>7.540200e-07</td>\n",
       "      <td>-1.659300e-07</td>\n",
       "      <td>0.013226</td>\n",
       "      <td>0.013230</td>\n",
       "      <td>0.013247</td>\n",
       "      <td>-0.015566</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.661710</td>\n",
       "      <td>3.301300</td>\n",
       "      <td>6.479100</td>\n",
       "      <td>-1.500300</td>\n",
       "      <td>-1.500300</td>\n",
       "      <td>-1.500300</td>\n",
       "      <td>-1.498100</td>\n",
       "      <td>-1.498100</td>\n",
       "      <td>-1.498000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.877700e-05</td>\n",
       "      <td>7.520000e-05</td>\n",
       "      <td>3.554700e-06</td>\n",
       "      <td>1.906200e-05</td>\n",
       "      <td>7.138600e-05</td>\n",
       "      <td>0.024770</td>\n",
       "      <td>0.024776</td>\n",
       "      <td>0.024777</td>\n",
       "      <td>0.020614</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.573980</td>\n",
       "      <td>8.288500</td>\n",
       "      <td>9.857500</td>\n",
       "      <td>-1.498200</td>\n",
       "      <td>-1.498200</td>\n",
       "      <td>-1.498200</td>\n",
       "      <td>-1.496200</td>\n",
       "      <td>-1.496300</td>\n",
       "      <td>-1.496200</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>6.860850e-05</td>\n",
       "      <td>2.965940e-04</td>\n",
       "      <td>1.701300e-05</td>\n",
       "      <td>6.978450e-05</td>\n",
       "      <td>2.891515e-04</td>\n",
       "      <td>0.069125</td>\n",
       "      <td>0.069130</td>\n",
       "      <td>0.069131</td>\n",
       "      <td>0.099751</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.361745</td>\n",
       "      <td>18.545800</td>\n",
       "      <td>17.989300</td>\n",
       "      <td>-1.490550</td>\n",
       "      <td>-1.490400</td>\n",
       "      <td>-1.490700</td>\n",
       "      <td>-1.491100</td>\n",
       "      <td>-1.491350</td>\n",
       "      <td>-1.491250</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          current_1     current_2     current_3     current_4     current_5  \\\n",
       "count  58509.000000  5.850900e+04  5.850900e+04  5.850900e+04  5.850900e+04   \n",
       "mean      -0.000003  1.756436e-06  1.111802e-06 -9.722586e-07  1.775487e-06   \n",
       "std        0.000008  3.199783e-05  1.520782e-04  7.956279e-06  3.234750e-05   \n",
       "min       -0.000021 -6.427550e-05 -2.937900e-04 -1.887580e-05 -6.547550e-05   \n",
       "25%       -0.000007 -1.444400e-05 -7.239600e-05 -5.417500e-06 -1.475300e-05   \n",
       "50%       -0.000003  8.804600e-07  5.137700e-07 -1.059100e-06  7.540200e-07   \n",
       "75%        0.000002  1.877700e-05  7.520000e-05  3.554700e-06  1.906200e-05   \n",
       "max        0.000015  6.860850e-05  2.965940e-04  1.701300e-05  6.978450e-05   \n",
       "\n",
       "          current_6     current_7     current_8     current_9    current_10  \\\n",
       "count  5.850900e+04  58509.000000  58509.000000  58509.000000  58509.000000   \n",
       "mean  -7.457535e-07      0.002854      0.002849      0.002849     -0.006349   \n",
       "std    1.490893e-04      0.033692      0.033700      0.033701      0.050735   \n",
       "min   -2.915565e-04     -0.086972     -0.087041     -0.086978     -0.111281   \n",
       "25%   -7.379100e-05     -0.019927     -0.019951     -0.019925     -0.032144   \n",
       "50%   -1.659300e-07      0.013226      0.013230      0.013247     -0.015566   \n",
       "75%    7.138600e-05      0.024770      0.024776      0.024777      0.020614   \n",
       "max    2.891515e-04      0.069125      0.069130      0.069131      0.099751   \n",
       "\n",
       "       ...    current_40    current_41    current_42    current_43  \\\n",
       "count  ...  58509.000000  58509.000000  58509.000000  58509.000000   \n",
       "mean   ...     -0.650427      5.730645      7.702923     -1.500891   \n",
       "std    ...      0.100670      5.781169      4.347205      0.003629   \n",
       "min    ...     -0.902350     -0.596830      0.320660     -1.510950   \n",
       "25%    ...     -0.715470      1.450300      4.436300     -1.503300   \n",
       "50%    ...     -0.661710      3.301300      6.479100     -1.500300   \n",
       "75%    ...     -0.573980      8.288500      9.857500     -1.498200   \n",
       "max    ...     -0.361745     18.545800     17.989300     -1.490550   \n",
       "\n",
       "         current_44    current_45    current_46    current_47    current_48  \\\n",
       "count  58509.000000  58509.000000  58509.000000  58509.000000  58509.000000   \n",
       "mean      -1.500915     -1.500809     -1.497787     -1.497814     -1.497710   \n",
       "std        0.003640      0.003600      0.003002      0.002984      0.002988   \n",
       "min       -1.511200     -1.510700     -1.504700     -1.504550     -1.504450   \n",
       "25%       -1.503400     -1.503200     -1.499600     -1.499600     -1.499500   \n",
       "50%       -1.500300     -1.500300     -1.498100     -1.498100     -1.498000   \n",
       "75%       -1.498200     -1.498200     -1.496200     -1.496300     -1.496200   \n",
       "max       -1.490400     -1.490700     -1.491100     -1.491350     -1.491250   \n",
       "\n",
       "             output  \n",
       "count  58509.000000  \n",
       "mean       6.000000  \n",
       "std        3.162305  \n",
       "min        1.000000  \n",
       "25%        3.000000  \n",
       "50%        6.000000  \n",
       "75%        9.000000  \n",
       "max       11.000000  \n",
       "\n",
       "[8 rows x 49 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# statistics of the data \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c325dee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 11 different classes in the output data \n",
    "# which are from 1 to 11 classes \n",
    "df['output'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6b3a226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     5319\n",
       "2     5319\n",
       "3     5319\n",
       "4     5319\n",
       "5     5319\n",
       "6     5319\n",
       "7     5319\n",
       "8     5319\n",
       "9     5319\n",
       "10    5319\n",
       "11    5319\n",
       "Name: output, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for the number of each output class labels \n",
    "# It is a blanced multi class dataset with each class is having equal number of output labels \n",
    "df['output'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f0aa3e",
   "metadata": {},
   "source": [
    "# classification - Linear model- SGD classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0d9031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing input and output labels\n",
    "X = df.drop(['output'],axis=1)\n",
    "y = df['output']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08ded24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5730026  0.54585804 0.47811763 ... 0.375      0.375      0.36742424]\n",
      " [0.66228185 0.44420547 0.50328617 ... 0.30882353 0.30681818 0.29924242]\n",
      " [0.49939872 0.45973556 0.47065977 ... 0.45588235 0.45833333 0.45075758]\n",
      " ...\n",
      " [0.41869175 0.62362286 0.32534757 ... 1.         1.         1.        ]\n",
      " [0.45803432 0.73932528 0.         ... 0.84558824 0.85984848 0.85984848]\n",
      " [0.33549635 0.87696412 0.62036912 ... 0.75       0.75378788 0.76893939]]\n"
     ]
    }
   ],
   "source": [
    "# importing Min Max scalar for the data transformation\n",
    "# Scaling is very important in the case of linear regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_X = scaler.fit_transform(X)\n",
    "print(scaled_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b12c552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing the dataset into train,test and validation sets\n",
    "X_train, X_rem, y_train, y_rem  = train_test_split(scaled_X, y, train_size = 0.5, random_state = 100)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size = 0.5, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac6d4b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The input training  data shape is (29254, 48)\n",
      " The  output training data shape is (29254,)\n",
      " The input validation  data shape is (14627, 48)\n",
      " The  output validation data shape is (14627,)\n",
      " The input testing  data shape is (14627, 48)\n",
      " The  output testing data shape is (14627,)\n"
     ]
    }
   ],
   "source": [
    "# The shape of training dataset\n",
    "print(f\" The input training  data shape is {X_train.shape}\")\n",
    "print(f\" The  output training data shape is {y_train.shape}\")\n",
    "print(f\" The input validation  data shape is {X_valid.shape}\")\n",
    "print(f\" The  output validation data shape is {y_valid.shape}\")\n",
    "# The shape of the test dataset \n",
    "print(f\" The input testing  data shape is {X_valid.shape}\")\n",
    "print(f\" The  output testing data shape is {y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c475b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, recall_score, accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01713ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random = SGDClassifier()\n",
    "random.fit(X_train,y_train)\n",
    "y_train_pred = random.predict(X_train)\n",
    "y_valid_pred = random.predict(X_valid)\n",
    "y_test_pred  = random.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b4512",
   "metadata": {},
   "source": [
    "### Accuracies for the random model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c22c2bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The training data accuracy for the best model is 0.6593286388186231\n",
      " The validation data accuracy for the best model is 0.6515348328433719\n",
      " The testing data accuracy for the best model is 0.6579163248564397\n"
     ]
    }
   ],
   "source": [
    "print(f\" The training data accuracy for the best model is {accuracy_score(y_train,y_train_pred)}\")\n",
    "print(f\" The validation data accuracy for the best model is {accuracy_score(y_valid,y_valid_pred)}\")\n",
    "print(f\" The testing data accuracy for the best model is {accuracy_score(y_test,y_test_pred)}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d1545e",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning with grid search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7088a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=SGDClassifier(),\n",
       "             param_grid={&#x27;alpha&#x27;: [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0,\n",
       "                                   1000.0],\n",
       "                         &#x27;loss&#x27;: [&#x27;log&#x27;], &#x27;n_jobs&#x27;: [-1], &#x27;penalty&#x27;: [&#x27;l2&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=SGDClassifier(),\n",
       "             param_grid={&#x27;alpha&#x27;: [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0,\n",
       "                                   1000.0],\n",
       "                         &#x27;loss&#x27;: [&#x27;log&#x27;], &#x27;n_jobs&#x27;: [-1], &#x27;penalty&#x27;: [&#x27;l2&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SGDClassifier(),\n",
       "             param_grid={'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0,\n",
       "                                   1000.0],\n",
       "                         'loss': ['log'], 'n_jobs': [-1], 'penalty': ['l2']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "#Hyperparameters\n",
    "grid = {\n",
    "    'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3], # learning rate\n",
    "    'loss': ['log'], \n",
    "    'penalty': ['l2'],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "\n",
    "sgd_cls = SGDClassifier()\n",
    "gs = GridSearchCV(sgd_cls,                    # model\n",
    "                   param_grid = grid,   # hyperparameters\n",
    "                   scoring='accuracy',        # metric for scoring\n",
    "                   cv=5)                  # number of folds\n",
    "gs.fit(X_valid,y_valid)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4886f124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.0001, 'loss': 'log', 'n_jobs': -1, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efeaa6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# best sgd classifier model \n",
    "best_sgd = SGDClassifier(alpha=0.0001,loss='log',n_jobs=-1,penalty='l2')\n",
    "best_sgd.fit(X_train,y_train)\n",
    "filename = 'best_sgd.sav'\n",
    "pickle.dump(best_sgd, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee0545",
   "metadata": {},
   "source": [
    "### accuracies of best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d2d1cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The training data accuracy for the best model is 0.750119641758392\n",
      " The validation data accuracy for the best model is 0.7469064059615779\n",
      " The testing data accuracy for the best model is 0.7491796554552912\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = best_sgd.predict(X_train)\n",
    "y_valid_pred = best_sgd.predict(X_valid)\n",
    "y_test_pred  = best_sgd.predict(X_test)\n",
    "print(f\" The training data accuracy for the best model is {accuracy_score(y_train,y_train_pred)}\")\n",
    "print(f\" The validation data accuracy for the best model is {accuracy_score(y_valid,y_valid_pred)}\")\n",
    "print(f\" The testing data accuracy for the best model is {accuracy_score(y_test,y_test_pred)}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d7455",
   "metadata": {},
   "source": [
    "### confusion matrix for best model for all sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edad7849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2492    0    0    0    0  126    0    0   66    0    0]\n",
      " [   2 1081  141  212   18   59    0    0   21 1029    0]\n",
      " [   0  194 1452  500  230  190    2    0    5  166    0]\n",
      " [   0   10   46 2489   14    0    4    0    0  125    0]\n",
      " [   0    6  227  656 1358   63    1  352   51    2    0]\n",
      " [ 543    7   22    0    9 1672    0    4  351    0    0]\n",
      " [   0    0    0    8    0    0 2678    0    0    0    0]\n",
      " [  40    1   28   80  321  163    0 1925   97    1    0]\n",
      " [  65    2   34   11   78  476    0    8 2023    2    0]\n",
      " [   0  248    9  181    2    1    0    0    0 2176    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0 2598]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix for training data\n",
    "print(confusion_matrix(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae115b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1228    0    0    0    0   67    0    1   36    0    0]\n",
      " [   0  512   92  128    8   29    0    0   11  609    0]\n",
      " [   0   96  672  254   97   93    2    0    4   72    0]\n",
      " [   0    1   17 1236    6    0    3    0    0   73    0]\n",
      " [   0    5  121  286  676   37    1  154   20    2    0]\n",
      " [ 273    3    6    0    5  881    0    3  169    0    0]\n",
      " [   0    0    0    4    0    0 1301    0    0    0    0]\n",
      " [  13    0   19   37  153   94    1  955   47    0    0]\n",
      " [  43    5   12    3   25  237    0    4  996    0    0]\n",
      " [   0  138    3   75    2    2    0    0    0 1122    1]\n",
      " [   0    0    0    0    0    0    0    0    0    0 1346]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix for training data\n",
    "print(confusion_matrix(y_valid,y_valid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1953985b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1205    0    0    0    0   61    0    2   35    0    0]\n",
      " [   0  531   76  119    6   40    0    0   19  575    1]\n",
      " [   0   99  714  231   85   86    2    0    1   72    0]\n",
      " [   0    1   16 1210    7    0    4    0    0   57    0]\n",
      " [   1    3  106  294  671   31    0  181   13    1    0]\n",
      " [ 303    5    7    0    9  859    0    3  185    0    0]\n",
      " [   0    0    0    4    0    0 1324    0    0    0    0]\n",
      " [  25    0   17   33  154   92    2  975   46    0    0]\n",
      " [  33    4   23    6   41  223    0    1  964    0    0]\n",
      " [   0  139    2   82    4    1    0    0    0 1131    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0 1375]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix for testing data\n",
    "print(confusion_matrix(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a5d1f",
   "metadata": {},
   "source": [
    "### classification report for the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a3f746f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.93      0.86      2684\n",
      "           2       0.70      0.42      0.53      2563\n",
      "           3       0.74      0.53      0.62      2739\n",
      "           4       0.60      0.93      0.73      2688\n",
      "           5       0.67      0.50      0.57      2716\n",
      "           6       0.61      0.64      0.62      2608\n",
      "           7       1.00      1.00      1.00      2686\n",
      "           8       0.84      0.72      0.78      2656\n",
      "           9       0.77      0.75      0.76      2699\n",
      "          10       0.62      0.83      0.71      2617\n",
      "          11       1.00      1.00      1.00      2598\n",
      "\n",
      "    accuracy                           0.75     29254\n",
      "   macro avg       0.76      0.75      0.74     29254\n",
      "weighted avg       0.76      0.75      0.74     29254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report for the training data\n",
    "print(classification_report(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b44a819a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.92      0.85      1332\n",
      "           2       0.67      0.37      0.48      1389\n",
      "           3       0.71      0.52      0.60      1290\n",
      "           4       0.61      0.93      0.74      1336\n",
      "           5       0.70      0.52      0.59      1302\n",
      "           6       0.61      0.66      0.63      1340\n",
      "           7       0.99      1.00      1.00      1305\n",
      "           8       0.85      0.72      0.78      1319\n",
      "           9       0.78      0.75      0.76      1325\n",
      "          10       0.60      0.84      0.70      1343\n",
      "          11       1.00      1.00      1.00      1346\n",
      "\n",
      "    accuracy                           0.75     14627\n",
      "   macro avg       0.76      0.75      0.74     14627\n",
      "weighted avg       0.76      0.75      0.74     14627\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report for the validation data\n",
    "print(classification_report(y_valid,y_valid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ff0c5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.92      0.84      1303\n",
      "           2       0.68      0.39      0.49      1367\n",
      "           3       0.74      0.55      0.63      1290\n",
      "           4       0.61      0.93      0.74      1295\n",
      "           5       0.69      0.52      0.59      1301\n",
      "           6       0.62      0.63      0.62      1371\n",
      "           7       0.99      1.00      1.00      1328\n",
      "           8       0.84      0.73      0.78      1344\n",
      "           9       0.76      0.74      0.75      1295\n",
      "          10       0.62      0.83      0.71      1359\n",
      "          11       1.00      1.00      1.00      1375\n",
      "\n",
      "    accuracy                           0.75     14628\n",
      "   macro avg       0.76      0.75      0.74     14628\n",
      "weighted avg       0.76      0.75      0.74     14628\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report for the testing data\n",
    "print(classification_report(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc58850",
   "metadata": {},
   "source": [
    "### Roc auc score for the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "200bb244",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_prob = best_sgd.predict_proba(X_train)\n",
    "y_valid_prob = best_sgd.predict_proba(X_valid)\n",
    "y_test_prob  = best_sgd.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c052f921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The roc_auc score for the training data is 0.9691666553922633\n",
      " The roc_auc score for the validation data is 0.9693981894782019\n",
      " The roc_auc score for the testing data is 0.9692134384375816\n"
     ]
    }
   ],
   "source": [
    "print(f\" The roc_auc score for the training data is {roc_auc_score(y_train,y_train_prob,average='weighted',multi_class='ovr')}\")\n",
    "print(f\" The roc_auc score for the validation data is {roc_auc_score(y_valid,y_valid_prob,average='weighted',multi_class='ovr')}\")\n",
    "print(f\" The roc_auc score for the testing data is {roc_auc_score(y_test,y_test_prob,average='weighted',multi_class='ovr')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff70e9b8",
   "metadata": {},
   "source": [
    "### Recall  scores for models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78696298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The recall score for the training data is 0.7500224879845168\n",
      " The recall score for the validation data is 0.747398696124673\n",
      " The recall score for the testing data is 0.747398696124673\n"
     ]
    }
   ],
   "source": [
    "# recall score for training data\n",
    "print(f\" The recall score for the training data is {recall_score(y_train,y_train_pred,average= 'macro')}\")\n",
    "# recall score for validation data\n",
    "print(f\" The recall score for the validation data is {recall_score(y_valid,y_valid_pred,average= 'macro')}\")\n",
    "# recall score for test data\n",
    "print(f\" The recall score for the testing data is {recall_score(y_valid,y_valid_pred,average= 'macro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35875d2a",
   "metadata": {},
   "source": [
    "### f1-score for the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7828dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The f-1 score for the training data is 0.7431666740126543\n",
      " The f-1 score for the validation data is 0.7386673858806371\n",
      " The f-1 score for the testing data is 0.7386673858806371\n"
     ]
    }
   ],
   "source": [
    "# F-1 score for training data\n",
    "print(f\" The f-1 score for the training data is {f1_score(y_train,y_train_pred,average= 'weighted')}\")\n",
    "# F-1 score for validation data\n",
    "print(f\" The f-1 score for the validation data is {f1_score(y_valid,y_valid_pred,average= 'weighted')}\")\n",
    "# F-1 score for test data\n",
    "print(f\" The f-1 score for the testing data is {f1_score(y_valid,y_valid_pred,average= 'weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60fac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01510be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9812f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06a0915b",
   "metadata": {},
   "source": [
    "# using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f7385d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The training data accuracy for the best model is 0.8962535037943529\n",
      " The validation data accuracy for the best model is 0.8949203527722704\n",
      " The testing data accuracy for the best model is 0.894927536231884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log = LogisticRegression()\n",
    "log.fit(X_train,y_train)\n",
    "y_train_pred = log.predict(X_train)\n",
    "y_valid_pred = log.predict(X_valid)\n",
    "y_test_pred = log.predict(X_test)\n",
    "print(f\" The training data accuracy for the best model is {accuracy_score(y_train,y_train_pred)}\")\n",
    "print(f\" The validation data accuracy for the best model is {accuracy_score(y_valid,y_valid_pred)}\")\n",
    "print(f\" The testing data accuracy for the best model is {accuracy_score(y_test,y_test_pred)}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "74c408ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "10 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\vasuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.79113941 0.88856164 0.8768708  0.7684419 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;, &#x27;liblinear&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;, &#x27;liblinear&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={'penalty': ['l1', 'l2'],\n",
       "                         'solver': ['newton-cg', 'lbfgs', 'liblinear']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# logistic regression hyper parameter validation\n",
    "parameters = {\n",
    "    'penalty' : ['l1','l2'], \n",
    "    'solver'  : ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "}\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "gs = GridSearchCV(logreg,                    # model\n",
    "                   param_grid = parameters,   # hyperparameters\n",
    "                   scoring='accuracy',        # metric for scoring\n",
    "                   cv=5)                  # number of folds\n",
    "gs.fit(X_valid,y_valid)                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d2e976ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'penalty': 'l2', 'solver': 'newton-cg'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "65725c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_log = LogisticRegression(penalty= 'l2',solver='newton-cg')\n",
    "best_log.fit(X_train,y_train)\n",
    "filename = 'best_log.sav'\n",
    "pickle.dump(best_log, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7116082a",
   "metadata": {},
   "source": [
    "### Accuracies of the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5d7b661d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The training data accuracy for the best model is 0.9095850140151774\n",
      " The validation data accuracy for the best model is 0.9085253298694196\n",
      " The testing data accuracy for the best model is 0.9079846869018321\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = best_log.predict(X_train)\n",
    "y_valid_pred = best_log.predict(X_valid)\n",
    "y_test_pred = best_log.predict(X_test)\n",
    "print(f\" The training data accuracy for the best model is {accuracy_score(y_train,y_train_pred)}\")\n",
    "print(f\" The validation data accuracy for the best model is {accuracy_score(y_valid,y_valid_pred)}\")\n",
    "print(f\" The testing data accuracy for the best model is {accuracy_score(y_test,y_test_pred)}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c1a101",
   "metadata": {},
   "source": [
    "### Recall score of the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1efc9758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The recall score for the training data is 0.9095806868150568\n",
      " The recall score for the validation data is 0.9085545233344728\n",
      " The recall score for the testing data is 0.9085545233344728\n"
     ]
    }
   ],
   "source": [
    "# recall score for training data\n",
    "print(f\" The recall score for the training data is {recall_score(y_train,y_train_pred,average= 'macro')}\")\n",
    "# recall score for validation data\n",
    "print(f\" The recall score for the validation data is {recall_score(y_valid,y_valid_pred,average= 'macro')}\")\n",
    "# recall score for test data\n",
    "print(f\" The recall score for the testing data is {recall_score(y_valid,y_valid_pred,average= 'macro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9cab88",
   "metadata": {},
   "source": [
    "### f1 score of the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "06a3e396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The f-1 score for the training data is 0.9090777181014986\n",
      " The f-1 score for the validation data is 0.9079970743606399\n",
      " The f-1 score for the testing data is 0.9079970743606399\n"
     ]
    }
   ],
   "source": [
    "# F-1 score for training data\n",
    "print(f\" The f-1 score for the training data is {f1_score(y_train,y_train_pred,average= 'weighted')}\")\n",
    "# F-1 score for validation data\n",
    "print(f\" The f-1 score for the validation data is {f1_score(y_valid,y_valid_pred,average= 'weighted')}\")\n",
    "# F-1 score for test data\n",
    "print(f\" The f-1 score for the testing data is {f1_score(y_valid,y_valid_pred,average= 'weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b8b946",
   "metadata": {},
   "source": [
    "### Roc_Auc_score of the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a939ea2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The roc_auc score for the training data is 0.9947233950917425\n",
      " The roc_auc score for the validation data is 0.9945940875405269\n",
      " The roc_auc score for the testing data is 0.9944914153014618\n"
     ]
    }
   ],
   "source": [
    "y_train_prob = best_log.predict_proba(X_train)\n",
    "y_valid_prob = best_log.predict_proba(X_valid)\n",
    "y_test_prob = best_log.predict_proba(X_test)\n",
    "print(f\" The roc_auc score for the training data is {roc_auc_score(y_train,y_train_prob,average='weighted',multi_class='ovr')}\")\n",
    "print(f\" The roc_auc score for the validation data is {roc_auc_score(y_valid,y_valid_prob,average='weighted',multi_class='ovr')}\")\n",
    "print(f\" The roc_auc score for the testing data is {roc_auc_score(y_test,y_test_prob,average='weighted',multi_class='ovr')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f14dc2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2578    0    0    0    0   96    0    0   10    0    0]\n",
      " [   1 2255    0    0    0    0    0    0    9  298    0]\n",
      " [   0    0 2557   60  121    1    0    0    0    0    0]\n",
      " [   0    0  100 2551   32    0    4    1    0    0    0]\n",
      " [   0    0  129   94 2120    0    0  372    0    1    0]\n",
      " [ 171    1    2    0    0 2169    0    2  263    0    0]\n",
      " [   0    0    0    2    0    0 2684    0    0    0    0]\n",
      " [   1    0   10   13  193    0    0 2438    0    1    0]\n",
      " [  21   13    8    0    0  305    0    1 2345    6    0]\n",
      " [   0  300    0    1    0    0    0    0    2 2314    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0 2598]]\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix of the training data\n",
    "print(confusion_matrix(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ecb1c5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1271    0    0    0    0   51    0    0   10    0    0]\n",
      " [   0 1241    1    0    0    0    0    0    2  145    0]\n",
      " [   0    0 1217   30   43    0    0    0    0    0    0]\n",
      " [   0    0   51 1266   15    0    4    0    0    0    0]\n",
      " [   0    0   72   47 1018    0    0  165    0    0    0]\n",
      " [  91    0    1    0    2 1109    0    2  135    0    0]\n",
      " [   0    0    0    2    0    0 1303    0    0    0    0]\n",
      " [   0    0    9    5   87    1    0 1217    0    0    0]\n",
      " [   4    4    6    0    0  175    0    0 1135    1    0]\n",
      " [   0  177    0    0    0    0    0    0    0 1166    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0 1346]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix for the validation data\n",
    "print(confusion_matrix(y_valid,y_valid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7f1088f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1254    0    0    0    0   39    0    0   10    0    0]\n",
      " [   0 1196    0    1    0    0    0    0    6  163    1]\n",
      " [   0    0 1201   27   60    0    0    0    2    0    0]\n",
      " [   0    0   42 1236   14    0    3    0    0    0    0]\n",
      " [   0    0   50   47 1017    0    0  187    0    0    0]\n",
      " [  96    0    0    0    1 1126    0    1  147    0    0]\n",
      " [   0    0    0    0    0    0 1328    0    0    0    0]\n",
      " [   0    0    3    5  101    0    1 1234    0    0    0]\n",
      " [   4    3    8    2    0  137    0    1 1137    3    0]\n",
      " [   0  180    0    0    0    0    0    0    1 1178    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0 1375]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix for the test data\n",
    "print(confusion_matrix(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3b62ac4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.96      0.95      2684\n",
      "           2       0.88      0.88      0.88      2563\n",
      "           3       0.91      0.93      0.92      2739\n",
      "           4       0.94      0.95      0.94      2688\n",
      "           5       0.86      0.78      0.82      2716\n",
      "           6       0.84      0.83      0.84      2608\n",
      "           7       1.00      1.00      1.00      2686\n",
      "           8       0.87      0.92      0.89      2656\n",
      "           9       0.89      0.87      0.88      2699\n",
      "          10       0.88      0.88      0.88      2617\n",
      "          11       1.00      1.00      1.00      2598\n",
      "\n",
      "    accuracy                           0.91     29254\n",
      "   macro avg       0.91      0.91      0.91     29254\n",
      "weighted avg       0.91      0.91      0.91     29254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report for training data\n",
    "print(classification_report(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb84902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
